[2025-09-24 16:31:54,310][baye_lora][INFO] - Using device: cuda:0
[2025-09-24 16:31:55,928][root][INFO] - Setting up PEFT
[2025-09-24 16:31:57,307][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 16:31:57,308][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 16:35:24,114][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-05 NLL=1.0651 | ACC=0.5980 | ECE=0.0789 (MC=2, train_mode=True)
[2025-09-24 16:35:36,906][baye_lora][INFO] - Using device: cuda:0
[2025-09-24 16:35:38,532][root][INFO] - Setting up PEFT
[2025-09-24 16:35:39,964][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 16:35:39,964][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 16:39:05,672][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9873 | ACC=0.6216 | ECE=0.0751 (MC=2, train_mode=True)
[2025-09-24 16:42:24,416][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-05 NLL=0.8705 | ACC=0.6588 | ECE=0.0342 (MC=2, train_mode=True)
[2025-09-24 16:45:26,190][baye_lora][INFO] - Using device: cuda:0
[2025-09-24 16:45:27,684][root][INFO] - Setting up PEFT
[2025-09-24 16:45:29,065][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 16:45:29,065][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 16:45:59,745][baye_lora][INFO] - Using device: cuda:0
[2025-09-24 16:46:01,270][root][INFO] - Setting up PEFT
[2025-09-24 16:46:02,661][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 16:46:02,661][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 16:46:14,866][baye_lora][INFO] - Using device: cuda:2
[2025-09-24 16:46:16,353][root][INFO] - Setting up PEFT
[2025-09-24 16:46:17,777][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 16:46:17,777][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 16:49:28,200][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=1.0253 | ACC=0.5676 | ECE=0.0867 (MC=2, train_mode=True)
[2025-09-24 16:49:43,831][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9307 | ACC=0.6655 | ECE=0.1118 (MC=2, train_mode=True)
[2025-09-24 16:52:46,355][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-05 NLL=0.8801 | ACC=0.6689 | ECE=0.0781 (MC=2, train_mode=True)
[2025-09-24 16:53:04,205][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-05 NLL=0.8270 | ACC=0.7027 | ECE=0.0665 (MC=2, train_mode=True)
[2025-09-24 16:56:19,834][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-05 NLL=0.8925 | ACC=0.6892 | ECE=0.1320 (MC=2, train_mode=True)
[2025-09-24 16:56:41,210][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-05 NLL=0.7900 | ACC=0.7230 | ECE=0.0972 (MC=2, train_mode=True)
[2025-09-24 16:58:42,823][baye_lora][INFO] - Using device: cuda:2
[2025-09-24 16:58:44,452][root][INFO] - Setting up PEFT
[2025-09-24 16:58:45,882][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 16:58:45,882][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 16:59:53,991][baye_lora][INFO] - [EP 3 | STEP 744] LR: 4.955000e-06 NLL=0.9348 | ACC=0.6892 | ECE=0.1574 (MC=2, train_mode=True)
[2025-09-24 17:02:11,737][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9583 | ACC=0.6318 | ECE=0.0822 (MC=2, train_mode=True)
[2025-09-24 17:03:27,350][baye_lora][INFO] - [EP 4 | STEP 930] LR: 4.955000e-06 NLL=0.9351 | ACC=0.6858 | ECE=0.1604 (MC=2, train_mode=True)
[2025-09-24 17:05:31,466][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-05 NLL=0.9273 | ACC=0.6318 | ECE=0.1066 (MC=2, train_mode=True)
[2025-09-24 17:07:00,487][baye_lora][INFO] - [EP 5 | STEP 1116] LR: 4.955000e-06 NLL=0.9438 | ACC=0.6858 | ECE=0.1563 (MC=2, train_mode=True)
[2025-09-24 17:07:00,487][baye_lora][INFO] - [FINAL] lr=4.955e-04, wd=2.056e-01 ECE=0.0781 | NLL=0.8801 | ACC=0.6689
[2025-09-24 17:09:06,683][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-05 NLL=0.8451 | ACC=0.6892 | ECE=0.1197 (MC=2, train_mode=True)
[2025-09-24 17:09:30,375][baye_lora][INFO] - Using device: cuda:2
[2025-09-24 17:09:31,845][root][INFO] - Setting up PEFT
[2025-09-24 17:09:33,246][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 17:09:33,246][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 17:12:58,349][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9721 | ACC=0.6486 | ECE=0.1132 (MC=2, train_mode=True)
[2025-09-24 17:16:17,307][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-05 NLL=0.9116 | ACC=0.6520 | ECE=0.0436 (MC=2, train_mode=True)
[2025-09-24 17:18:11,376][baye_lora][INFO] - Using device: cuda:1
[2025-09-24 17:18:13,054][root][INFO] - Setting up PEFT
[2025-09-24 17:18:14,446][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 17:18:14,446][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 17:19:52,757][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-05 NLL=0.8397 | ACC=0.7128 | ECE=0.1179 (MC=2, train_mode=True)
[2025-09-24 17:22:32,914][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9289 | ACC=0.6588 | ECE=0.1183 (MC=4, train_mode=True)
[2025-09-24 17:22:36,075][baye_lora][INFO] - Using device: cuda:1
[2025-09-24 17:22:37,705][root][INFO] - Setting up PEFT
[2025-09-24 17:22:39,127][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 17:22:39,127][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 17:29:41,731][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-05 NLL=0.9257 | ACC=0.6622 | ECE=0.0426 (MC=4, train_mode=True)
[2025-09-24 17:30:10,261][baye_lora][INFO] - Using device: cuda:0
[2025-09-24 17:30:11,887][root][INFO] - Setting up PEFT
[2025-09-24 17:30:13,294][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 17:30:13,295][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 17:34:17,806][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-05 NLL=0.8647 | ACC=0.7095 | ECE=0.1168 (MC=4, train_mode=True)
[2025-09-24 17:34:30,097][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9843 | ACC=0.6250 | ECE=0.0949 (MC=4, train_mode=True)
[2025-09-24 17:38:40,341][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-04 NLL=0.8966 | ACC=0.6419 | ECE=0.0788 (MC=4, train_mode=True)
[2025-09-24 17:38:45,803][baye_lora][INFO] - [EP 3 | STEP 744] LR: 4.955000e-06 NLL=0.9021 | ACC=0.7027 | ECE=0.1274 (MC=4, train_mode=True)
[2025-09-24 17:43:05,505][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-04 NLL=1.0358 | ACC=0.6520 | ECE=0.1626 (MC=4, train_mode=True)
[2025-09-24 17:43:13,460][baye_lora][INFO] - [EP 4 | STEP 930] LR: 4.955000e-06 NLL=0.8996 | ACC=0.6993 | ECE=0.1257 (MC=4, train_mode=True)
[2025-09-24 17:43:57,245][baye_lora][INFO] - Using device: cuda:0
[2025-09-24 17:43:58,872][root][INFO] - Setting up PEFT
[2025-09-24 17:44:00,244][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 17:44:00,245][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 17:47:42,146][baye_lora][INFO] - [EP 5 | STEP 1116] LR: 4.955000e-06 NLL=0.9026 | ACC=0.7027 | ECE=0.1258 (MC=4, train_mode=True)
[2025-09-24 17:47:42,147][baye_lora][INFO] - [FINAL] lr=4.955e-04, wd=2.056e-01 ECE=0.0426 | NLL=0.9257 | ACC=0.6622
[2025-09-24 17:48:17,209][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9562 | ACC=0.6419 | ECE=0.1066 (MC=4, train_mode=True)
[2025-09-24 17:52:26,687][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-04 NLL=0.8596 | ACC=0.6858 | ECE=0.0673 (MC=4, train_mode=True)
[2025-09-24 17:56:36,617][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-04 NLL=0.8989 | ACC=0.6993 | ECE=0.1052 (MC=4, train_mode=True)
[2025-09-24 17:58:41,274][baye_lora][INFO] - Using device: cuda:4
[2025-09-24 17:58:42,914][root][INFO] - Setting up PEFT
[2025-09-24 17:58:44,373][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 17:58:44,373][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 18:00:47,178][baye_lora][INFO] - [EP 3 | STEP 744] LR: 4.955000e-05 NLL=1.0400 | ACC=0.6588 | ECE=0.1843 (MC=4, train_mode=True)
[2025-09-24 18:02:59,991][baye_lora][INFO] - [EP 0 | STEP 186] LR: 4.955000e-04 NLL=0.9961 | ACC=0.6081 | ECE=0.0785 (MC=4, train_mode=True)
[2025-09-24 18:05:12,308][baye_lora][INFO] - [EP 4 | STEP 930] LR: 4.955000e-05 NLL=0.9279 | ACC=0.7027 | ECE=0.1337 (MC=4, train_mode=True)
[2025-09-24 18:07:08,259][baye_lora][INFO] - [EP 1 | STEP 372] LR: 4.955000e-05 NLL=0.8881 | ACC=0.6926 | ECE=0.0775 (MC=4, train_mode=True)
[2025-09-24 18:09:37,871][baye_lora][INFO] - [EP 5 | STEP 1116] LR: 4.955000e-06 NLL=0.9552 | ACC=0.7095 | ECE=0.1485 (MC=4, train_mode=True)
[2025-09-24 18:11:16,497][baye_lora][INFO] - [EP 2 | STEP 558] LR: 4.955000e-05 NLL=0.8347 | ACC=0.7095 | ECE=0.1171 (MC=4, train_mode=True)
[2025-09-24 18:14:03,621][baye_lora][INFO] - [EP 6 | STEP 1302] LR: 4.955000e-06 NLL=0.9585 | ACC=0.7128 | ECE=0.1452 (MC=4, train_mode=True)
[2025-09-24 18:15:25,081][baye_lora][INFO] - [EP 3 | STEP 744] LR: 4.955000e-06 NLL=0.8481 | ACC=0.7095 | ECE=0.1303 (MC=4, train_mode=True)
[2025-09-24 18:15:48,746][baye_lora][INFO] - Using device: cuda:2
[2025-09-24 18:15:50,275][root][INFO] - Setting up PEFT
[2025-09-24 18:15:51,674][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 18:15:51,674][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 18:19:34,953][baye_lora][INFO] - [EP 4 | STEP 930] LR: 4.955000e-06 NLL=0.8500 | ACC=0.7196 | ECE=0.1188 (MC=4, train_mode=True)
[2025-09-24 18:23:43,884][baye_lora][INFO] - [EP 5 | STEP 1116] LR: 4.955000e-06 NLL=0.8504 | ACC=0.7264 | ECE=0.1176 (MC=4, train_mode=True)
[2025-09-24 18:23:44,636][baye_lora][INFO] - [EP 0 | STEP 375] LR: 5.456000e-04 NLL=0.4780 | ACC=0.8697 | ECE=0.1192 (MC=4, train_mode=True)
[2025-09-24 18:28:08,501][baye_lora][INFO] - [EP 6 | STEP 1302] LR: 4.955000e-06 NLL=0.8550 | ACC=0.7264 | ECE=0.1190 (MC=4, train_mode=True)
[2025-09-24 18:31:33,123][baye_lora][INFO] - [EP 1 | STEP 750] LR: 5.456000e-05 NLL=0.5218 | ACC=0.8468 | ECE=0.0705 (MC=4, train_mode=True)
[2025-09-24 18:32:32,180][baye_lora][INFO] - [EP 7 | STEP 1488] LR: 4.955000e-06 NLL=0.8617 | ACC=0.7264 | ECE=0.1178 (MC=4, train_mode=True)
[2025-09-24 18:36:56,245][baye_lora][INFO] - [EP 8 | STEP 1674] LR: 4.955000e-06 NLL=0.8618 | ACC=0.7264 | ECE=0.1165 (MC=4, train_mode=True)
[2025-09-24 18:39:51,223][baye_lora][INFO] - [EP 2 | STEP 1125] LR: 5.456000e-05 NLL=0.4380 | ACC=0.8662 | ECE=0.0470 (MC=4, train_mode=True)
[2025-09-24 18:41:20,370][baye_lora][INFO] - [EP 9 | STEP 1860] LR: 4.955000e-06 NLL=0.8695 | ACC=0.7264 | ECE=0.1192 (MC=4, train_mode=True)
[2025-09-24 18:41:20,371][baye_lora][INFO] - [FINAL] lr=4.955e-04, wd=2.056e-01 ECE=0.0775 | NLL=0.8881 | ACC=0.6926
[2025-09-24 18:48:07,912][baye_lora][INFO] - [EP 3 | STEP 1500] LR: 5.456000e-06 NLL=0.4439 | ACC=0.8697 | ECE=0.0405 (MC=4, train_mode=True)
[2025-09-24 18:56:24,518][baye_lora][INFO] - [EP 4 | STEP 1875] LR: 5.456000e-06 NLL=0.4475 | ACC=0.8680 | ECE=0.0445 (MC=4, train_mode=True)
[2025-09-24 19:04:41,596][baye_lora][INFO] - [EP 5 | STEP 2250] LR: 5.456000e-06 NLL=0.4478 | ACC=0.8680 | ECE=0.0385 (MC=4, train_mode=True)
[2025-09-24 19:04:41,596][baye_lora][INFO] - [FINAL] lr=5.456e-04, wd=2.707e-02 ECE=0.1192 | NLL=0.4780 | ACC=0.8697
[2025-09-24 19:44:05,938][baye_lora][INFO] - Using device: cuda:2
[2025-09-24 19:44:07,409][root][INFO] - Setting up PEFT
[2025-09-24 19:44:09,033][baye_lora][INFO] - [Inject] replaced Linear layers: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): InducingLinear(
                    in_features=4096, out_features=8, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_B): ModuleDict(
                  (default): InducingLinear(
                    in_features=8, out_features=4096, bias=False
                    (row_flow): MaskedAffineAutoregressiveTransform(
                      (autoregressive_net): MADE(
                        (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                        (blocks): ModuleList(
                          (0-1): 2 x MaskedResidualBlock(
                            (linear_layers): ModuleList(
                              (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                            )
                            (dropout): Dropout(p=0.1, inplace=False)
                          )
                        )
                        (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
                      )
                    )
                  )
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): InducingLinear(
            in_features=4096, out_features=8, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_B): ModuleDict(
          (default): InducingLinear(
            in_features=8, out_features=32000, bias=False
            (row_flow): MaskedAffineAutoregressiveTransform(
              (autoregressive_net): MADE(
                (initial_layer): MaskedLinear(in_features=9, out_features=16, bias=True)
                (blocks): ModuleList(
                  (0-1): 2 x MaskedResidualBlock(
                    (linear_layers): ModuleList(
                      (0-1): 2 x MaskedLinear(in_features=16, out_features=16, bias=True)
                    )
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
                (final_layer): MaskedLinear(in_features=16, out_features=18, bias=True)
              )
            )
          )
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
        (lora_magnitude_vector): ModuleDict()
      )
    )
  )
)
[2025-09-24 19:44:09,033][baye_lora][INFO] - [Params] trainable=4.90M / total=6743.32M
[2025-09-24 19:52:02,341][baye_lora][INFO] - [EP 0 | STEP 375] LR: 7.393798e-04 NLL=0.6000 | ACC=0.8028 | ECE=0.0540 (MC=4, train_mode=True)
[2025-09-24 19:59:49,050][baye_lora][INFO] - [EP 1 | STEP 750] LR: 7.393798e-05 NLL=0.6220 | ACC=0.8028 | ECE=0.0410 (MC=4, train_mode=True)
[2025-09-24 20:08:06,839][baye_lora][INFO] - [EP 2 | STEP 1125] LR: 7.393798e-05 NLL=0.4433 | ACC=0.8732 | ECE=0.0380 (MC=4, train_mode=True)
[2025-09-24 20:16:24,120][baye_lora][INFO] - [EP 3 | STEP 1500] LR: 7.393798e-06 NLL=0.4602 | ACC=0.8803 | ECE=0.0433 (MC=4, train_mode=True)
[2025-09-24 20:24:40,831][baye_lora][INFO] - [EP 4 | STEP 1875] LR: 7.393798e-06 NLL=0.4684 | ACC=0.8785 | ECE=0.0473 (MC=4, train_mode=True)
[2025-09-24 20:32:58,100][baye_lora][INFO] - [EP 5 | STEP 2250] LR: 7.393798e-06 NLL=0.4715 | ACC=0.8785 | ECE=0.0463 (MC=4, train_mode=True)
[2025-09-24 20:32:58,101][baye_lora][INFO] - [FINAL] lr=7.394e-04, wd=2.707e-02 ECE=0.0380 | NLL=0.4433 | ACC=0.8732
